{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['POGR', 'FIVE', 'OZON', 'MAGN', 'MAIL', 'DSKY', 'TATN', 'TATNP', 'HYDR', 'HHRU', 'TCSG', 'PHOR', 'GLTR', 'GMKN', 'CBOM', 'RUAL', 'YNDX', 'NVTK', 'MOEX', 'ALRS', 'QIWI', 'POLY', 'RSTI', 'AFKS', 'IRAO', 'MGNT', 'VTBR', 'FEES', 'PIKK', 'NLMK', 'LKOH', 'PLZL', 'UPRO', 'LSRG', 'ROSN', 'GAZP', 'MTSS', 'CHMF', 'TRNFP', 'AFLT', 'SBERP', 'SBER', 'RTKM'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'C:/Users/makov/OneDrive/Desktop/RL project/data/charts' \n",
    "levels_path = 'C:/Users/makov/OneDrive/Desktop/RL project/data/supports_resistances.data' \n",
    "\n",
    "actives_data = {}\n",
    "for filename in glob.glob(os.path.join(path, '*.csv')):\n",
    "   with open(os.path.join(os.getcwd(), filename), 'r') as f: \n",
    "      active_name = filename.split(\"\\\\\")[-1].split(\".\")[0].split(\"_\")[0]\n",
    "      actives_data[active_name] = f.readlines()\n",
    "\n",
    "with open(levels_path, 'r') as f: \n",
    "      levels_lines = f.readlines()\n",
    "      levels_data = {}\n",
    "      active = \"init\"\n",
    "      tmp_arr = []\n",
    "      for line in levels_lines:\n",
    "          tmp = line.split(\"\\n\")[0].split(\",\")\n",
    "          if len(tmp) == 1:\n",
    "                pre_active = line.split(\"\\n\")[0].split(\" \")[0]\n",
    "                if pre_active == \"\":\n",
    "                     continue \n",
    "                levels_data[active] = tmp_arr if  len(tmp_arr) == 0 else np.stack(tmp_arr) \n",
    "                active = pre_active\n",
    "                tmp_arr = []\n",
    "          else:\n",
    "                tmp_arr.append(np.array(tmp))\n",
    "      _ = levels_data.pop(\"init\")\n",
    "              \n",
    "levels_data.keys()         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "active = actives_data[\"CBOM\"]\n",
    "levels = levels_data[\"CBOM\"]\n",
    "# active, levels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_level_life_time(price, date, levels_df, band_for_level):\n",
    "    observed_levels = levels_df[levels_df[\"date\"] <= date].reset_index()\n",
    "    best_locality = 0\n",
    "    best_level_date = None\n",
    "\n",
    "    for i in range(len(observed_levels)):\n",
    "        level = observed_levels.loc[i]\n",
    "        if np.abs((float(level[\"lower_bound\"]) - price) / price) < band_for_level or np.abs((float(level[\"upper_bound\"]) - price) / price) < band_for_level or float(level[\"lower_bound\"]) < price < float(level[\"upper_bound\"]):\n",
    "            if int(level[\"locality\"]) > best_locality and (date - level[\"date\"]).days <= 365:\n",
    "                best_locality = int(level[\"locality\"])\n",
    "                best_level_date = level[\"date\"]\n",
    "\n",
    "    result = 0.0\n",
    "    if best_level_date is not None:\n",
    "        result = (date - best_level_date).days / 365\n",
    "    if result > 1.0:\n",
    "        result = 0.0\n",
    "    return result\n",
    "            \n",
    "\n",
    "def prepara_data_for_one_timestamp(lines, levels, band_for_level = 2 / 100, scaler_for_d = 10):\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        data.append(np.array(line.split(\"\\n\")[0].split(\",\")).astype(\"float\"))\n",
    "\n",
    "    levels_data = pd.DataFrame(levels, columns = [\"date\", \"lower_bound\", \n",
    "                                                  \"upper_bound\", \"locality\", \n",
    "                                                  \"strength\", \"weakness\"])\n",
    "    levels_data = levels_data.drop([\"strength\", \"weakness\"], axis = 1)\n",
    "    levels_data[\"date\"] = pd.to_datetime(levels_data[\"date\"], format = \"%Y/%m/%d\" )\n",
    "    \n",
    "    # current open price = O = P2\n",
    "    # current close price = C = P3\n",
    "    # current max price = H = Pmax\n",
    "    # current min price = L = Pmin\n",
    "    # previous close price = P1\n",
    "\n",
    "    data = pd.DataFrame(data, columns = [\"date\", \"time\", \"P2\", \"Pmax\", \"Pmin\", \"P3\", \"V\"])\n",
    "    data = data.sort_values(by = [\"date\"])\n",
    "    data[\"date\"] = pd.to_datetime(data[\"date\"], format = \"%Y%m%d\" )\n",
    "    data = data.drop(\"time\", axis = 1)\n",
    "    data[\"day_of_week\"] = data[\"date\"].dt.day_of_week\n",
    "    data = data.drop(data[data[\"day_of_week\"] >=5].index ).reset_index()\n",
    "    data = data.join(pd.get_dummies(data[\"day_of_week\"], prefix = \"day_of_week\"))\n",
    "\n",
    "    level_life_times = []\n",
    "    for i in range(len(data)):\n",
    "        price = data.loc[i][\"P3\"]\n",
    "        date = data.loc[i][\"date\"]\n",
    "        level_life_times.append(get_level_life_time(price, date, levels_data, band_for_level))\n",
    "   \n",
    "    data[\"level_lifetime\"] = level_life_times\n",
    "\n",
    "\n",
    "    data[\"P1\"]    = [data[\"P3\"].values[0]] + list(data[\"P3\"].values)[:-1]\n",
    "    data[\"prevV\"] = [data[\"V\"].values[0]]  + list(data[\"V\"].values)[:-1]\n",
    "\n",
    "    data[\"D1\"] = (data[\"P2\"] - data[\"P1\"]) / data[\"P1\"] * scaler_for_d\n",
    "    data[\"D2\"] = (data[\"P3\"] - data[\"P2\"]) / data[\"P2\"] * scaler_for_d\n",
    "    data[\"D3\"] = (data[\"Pmax\"] - data[\"P2\"]) / data[\"P2\"] * scaler_for_d\n",
    "    data[\"D4\"] = (data[\"P2\"] - data[\"Pmin\"]) / data[\"Pmin\"] * scaler_for_d\n",
    "    data[\"D\"]  = (data[\"P3\"] - data[\"P1\"]) / data[\"P1\"] * scaler_for_d\n",
    "    data[\"VD\"] = (data[\"V\"] - data[\"prevV\"]) / data[\"prevV\"]\n",
    "\n",
    "    \n",
    "    data = data.drop([\"index\", #\"date\", \n",
    "                      \"day_of_week\", \n",
    "                      \"P1\", \"P2\", \"P3\", \n",
    "                      \"Pmin\", \"Pmax\", \n",
    "                      \"V\", \"prevV\"], axis = 1)\n",
    "    \n",
    "    return data, levels_data\n",
    "\n",
    "def prepare_active_data(lines, levels, N, M, band_for_level = 2 / 100, scaler_for_d = 10):\n",
    "\n",
    "    prepared_oneday_active, _ = prepara_data_for_one_timestamp(lines, levels, band_for_level = 2 / 100, scaler_for_d = 10)\n",
    "    \n",
    "    VDs = prepared_oneday_active[\"VD\"].values\n",
    "    Ds = prepared_oneday_active[\"D\"].values\n",
    "\n",
    "    Ds = np.stack([np.array([None for i in range(N)]) for j in range(N)] + [Ds[i-N:i] for  i in range(N, len(Ds))])\n",
    "    VDs = np.stack([np.array([None for i in range(M)]) for j in range(M)] + [VDs[i-M:i] for  i in range(M, len(VDs))])\n",
    "    # print(Ds.shape, VDs.shape)\n",
    "\n",
    "    prepared_oneday_active[[\"Ds_\" + str(i) for i in range(N)]] = Ds\n",
    "    prepared_oneday_active[[\"VDs_\" + str(i) for i in range(M)]] = VDs\n",
    "    return prepared_oneday_active[[\"date\"] +\n",
    "                                  [\"day_of_week_\" + str(i) for i in range(5)] +\n",
    "                                  [\"level_lifetime\"] +\n",
    "                                  [\"VDs_\" + str(i) for i in range(M)] +\n",
    "                                  [\"Ds_\" + str(i) for i in range(N)] + \n",
    "                                  [\"D3\", \"D4\", \"D1\", \"D2\"]].dropna()\n",
    "data = prepare_active_data(active, levels, N = 60, M = 10, band_for_level = 2 / 100, scaler_for_d = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 1996)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.columns), len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 36/45 [08:39<03:04, 20.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNGSP\n",
      "'SNGSP'\n",
      "SNGS\n",
      "'SNGS'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [10:14<00:00, 13.66s/it]\n"
     ]
    }
   ],
   "source": [
    "full_data_dict = {}\n",
    "for key in tqdm(actives_data.keys()):\n",
    "    try:\n",
    "        active = actives_data[key]\n",
    "        levels = levels_data[key]\n",
    "\n",
    "        full_data_dict[key] = prepare_active_data(active, levels, N = 60, M = 10, band_for_level = 1 / 100, scaler_for_d = 10)\n",
    "    except Exception as ex:\n",
    "        print(key)\n",
    "        print(ex)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'SNGS', 'SNGSP'}, set())"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(actives_data.keys()).difference(set(levels_data.keys())), set(levels_data.keys()).difference(set(actives_data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 45)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(actives_data.keys()), len(actives_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save = \"data/prepocessed_charts/\"\n",
    "for key in full_data_dict.keys():\n",
    "    full_data_dict[key].to_csv(path_to_save + key + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_data_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actives_for_test = [\"LKOH\", \"GAZP\", \"ROSN\"]\n",
    "actives_for_train = set(full_data_dict.keys()) - set(actives_for_test)\n",
    "len(actives_for_train), len(actives_for_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:13<00:00,  3.01it/s]\n"
     ]
    }
   ],
   "source": [
    "test_year = 2022\n",
    "with open(\"train.data\", \"w\") as fin:\n",
    "    for active in tqdm(actives_for_train):\n",
    "\n",
    "        active_data = full_data_dict[active].copy()\n",
    "        active_data = active_data[active_data[\"date\"].dt.year < test_year]\n",
    "        active_data = active_data.drop(\"date\", axis = 1)\n",
    "\n",
    "        fin.write(active + \" D \" + str(len(active_data)) + \"\\n\" )\n",
    "        for x in active_data.values:\n",
    "            tmp1 = np.array2string(\n",
    "                x[:5], \n",
    "                separator = \",\" , \n",
    "                max_line_width = 1e10,\n",
    "                formatter={\"all\": lambda x: \"%i\" %float(x)}\n",
    "                )[1:-1]\n",
    "            tmp2 = np.array2string(\n",
    "                    x[6:],\n",
    "                    separator = \",\" ,\n",
    "                    max_line_width = 1e10,\n",
    "                    formatter={\"all\": lambda x: \"%+.6f\" %float(x)}\n",
    "                    )[1:-1]\n",
    "            fin.write(tmp1 + \",\" + \"%.6f\" %float(x[5]) + \",\" +  tmp2 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for active in actives_for_test:\n",
    "    with open(f\"test_{active}.data\", \"w\") as fin:\n",
    "        active_data = full_data_dict[active].copy()\n",
    "        # active_data = active_data[active_data[\"date\"].dt.year > test_year]\n",
    "        active_data = active_data.drop(\"date\", axis = 1)\n",
    "\n",
    "        fin.write(active + \" D \" + str(len(active_data)) + \"\\n\" )\n",
    "        for x in active_data.values:\n",
    "            tmp1 = np.array2string(\n",
    "                x[:5], \n",
    "                separator = \",\" , \n",
    "                max_line_width = 1e10,\n",
    "                formatter={\"all\": lambda x: \"%i\" %float(x)}\n",
    "                )[1:-1]\n",
    "            tmp2 = np.array2string(\n",
    "                    x[6:],\n",
    "                    separator = \",\" ,\n",
    "                    max_line_width = 1e10,\n",
    "                    formatter={\"all\": lambda x: \"%+.6f\" %float(x)}\n",
    "                    )[1:-1]\n",
    "            fin.write(tmp1 + \",\" + \"%.6f\" %float(x[5]) + \",\" +  tmp2 + \"\\n\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for active in actives_for_test:\n",
    "    with open(f\"test_{active}_last.data\", \"w\") as fin:\n",
    "        active_data = full_data_dict[active].copy()\n",
    "        active_data = active_data[active_data[\"date\"].dt.year > test_year]\n",
    "        active_data = active_data.drop(\"date\", axis = 1)\n",
    "\n",
    "        fin.write(active + \" D \" + str(len(active_data)) + \"\\n\" )\n",
    "        for x in active_data.values:\n",
    "            tmp1 = np.array2string(\n",
    "                x[:5], \n",
    "                separator = \",\" , \n",
    "                max_line_width = 1e10,\n",
    "                formatter={\"all\": lambda x: \"%i\" %float(x)}\n",
    "                )[1:-1]\n",
    "            tmp2 = np.array2string(\n",
    "                    x[6:],\n",
    "                    separator = \",\" ,\n",
    "                    max_line_width = 1e10,\n",
    "                    formatter={\"all\": lambda x: \"%+.6f\" %float(x)}\n",
    "                    )[1:-1]\n",
    "            fin.write(tmp1 + \",\" + \"%.6f\" %float(x[5]) + \",\" +  tmp2 + \"\\n\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
